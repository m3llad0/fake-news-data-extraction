{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/m3llad0/fake-news-data-extraction/blob/main/Fake_news_detection_model_V1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBQrkFzbwDER"
      },
      "source": [
        "# Modelo de Detecci√≥n de Noticias Falsas en Espa√±ol\n",
        "\n",
        "Este notebook implementa un sistema h√≠brido basado en modelos de lenguaje (BETO) y Graph Neural Networks (GNNs) para detectar noticias falsas en espa√±ol. Se justifica el uso de una arquitectura h√≠brida por la necesidad de capturar tanto patrones ling√º√≠sticos (BETO) como metadatos y relaciones contextuales (GNN)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Configuraci√≥n del entorno y librer√≠as\n",
        "\n",
        "Se importan las librer√≠as necesarias para construir el modelo de detecci√≥n de noticias falsas.\n",
        "\n",
        "- `torch`: framework principal para redes neuronales y entrenamiento en GPU.\n",
        "- `transformers`: permite cargar modelos preentrenados como BETO desde HuggingFace.\n",
        "- `numpy`, `pandas`: utilizadas para manipulaci√≥n de datos.\n",
        "- `requests`: utilizada para acceder al dataset alojado v√≠a una API externa.\n",
        "\n",
        "Tambi√©n se verifica la disponibilidad de GPU, lo cual es relevante para acelerar el entrenamiento de modelos grandes como BETO. Si se dispone de una GPU (por ejemplo, Tesla T4 en Colab), el entrenamiento y la inferencia ser√°n considerablemente m√°s r√°pidos."
      ],
      "metadata": {
        "id": "Zs4owuEmCaZZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "C1Z0wYMil7c4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YETKo-iGOtyW",
        "outputId": "8f5ac068-1b93-437a-ea97-7a7bfe4e0c11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.8.0+cu126\n",
            "GPU Available: True\n",
            "GPU Name: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"GPU Available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wHPUrPiwZOR"
      },
      "source": [
        "## 1. Carga de datos\n",
        "\n",
        "El conjunto de datos se obtiene mediante una solicitud HTTP (`GET`) a una API propia alojada en Azure. Esta API expone una lista de noticias previamente recolectadas y etiquetadas, bajo el campo `\"Scrapped news\"`.\n",
        "\n",
        "Cada noticia contiene:\n",
        "- `TITULO`: encabezado de la noticia.\n",
        "- `CORPUS`: cuerpo del texto.\n",
        "- `VERACIDAD`: etiqueta binaria (`'true'` o `'false'`).\n",
        "\n",
        "### 1.1 Preprocesamiento\n",
        "Se realiza un preprocesamiento inicial para construir el texto de entrada y la etiqueta de clase:\n",
        "- Se concatena el t√≠tulo y el cuerpo para formar una sola secuencia (`text`).\n",
        "- Se mapea la etiqueta `VERACIDAD` a valores num√©ricos:  \n",
        "  - `0` para noticias reales  \n",
        "  - `1` para noticias falsas\n",
        "\n",
        "Este formato es necesario para alimentar el modelo de lenguaje que se usar√° en las siguientes etapas.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "RZSfx5xmwcCA"
      },
      "outputs": [],
      "source": [
        "\n",
        "# API endpoint\n",
        "ROUTE = \"https://my-thesis-aaacd0bxgzfae8a0.westus-01.azurewebsites.net/dataset\"\n",
        "\n",
        "# Fetch the data\n",
        "response = requests.get(ROUTE)\n",
        "data = response.json()\n",
        "\n",
        "# Extract 'Scrapped news' list\n",
        "news_items = data[\"Scrapped news\"]\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(news_items)\n",
        "\n",
        "# Preprocess the dataset for model input\n",
        "df['text'] = df['TITULO'] + \". \" + df['CORPUS']\n",
        "df['label'] = df['VERACIDAD'].map({'true': 0, 'false': 1, \"satira\":2})  # Map veracity to binary labels\n",
        "\n",
        "\n",
        "# Final dataset ready for model\n",
        "dataset = df[['text', 'label']]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "WNo0o2wXL2FX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "033c801c-9f62-4925-b0a5-24de1f735e5e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text  label\n",
              "0  Oxford lanza sus propios ex√°menes de certifica...      0\n",
              "1  La RAE estudia incluir ¬´machirulo¬ª en el Dicci...      0\n",
              "2  Realizan paro en Facultad de Ciencias Pol√≠tica...      0\n",
              "3  Deniegan el B1 a un joven mudo por no poder ap...      1\n",
              "4  Jorge Vergara recapacita y ofrece precios m√°s ...      1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b16bda6d-582e-4905-971c-ec8963a7ee1e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Oxford lanza sus propios ex√°menes de certifica...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>La RAE estudia incluir ¬´machirulo¬ª en el Dicci...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Realizan paro en Facultad de Ciencias Pol√≠tica...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Deniegan el B1 a un joven mudo por no poder ap...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Jorge Vergara recapacita y ofrece precios m√°s ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b16bda6d-582e-4905-971c-ec8963a7ee1e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b16bda6d-582e-4905-971c-ec8963a7ee1e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b16bda6d-582e-4905-971c-ec8963a7ee1e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-327c2d61-bf18-403f-aa59-4d1f75419380\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-327c2d61-bf18-403f-aa59-4d1f75419380')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-327c2d61-bf18-403f-aa59-4d1f75419380 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "dataset",
              "summary": "{\n  \"name\": \"dataset\",\n  \"rows\": 1298,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1017,\n        \"samples\": [\n          \"Chilangos que se recuperan de covid est\\u00e1n adquiriendo superpoderes. Debido a la pandemia de covid, era de esperarse que la vida de la ciudadan\\u00eda cambiara, pues la mayor\\u00eda de la gente no estaba acostumbrada a tanto encierro.\\n\\nEn el caso particular de los chilangos, esos cambios no fueron de la manera en que lo esperaban. Al parecer, la mezcla de tamalina \\u2014compuesto activo presente en los tamales\\u2014, las vacunas administradas, y el mismo virus, han hecho que muchos de los habitantes de la ciudad hayan comenzado a desarrollar superpoderes.\\n\\nAntonio Estrada, investigador de la UACM, ha sido quien descubri\\u00f3 este fen\\u00f3meno, y tuvo a bien compartirlo con la redacci\\u00f3n de El Dizque.\\n\\n\\u00abLa particular dieta de los chilangos, combinada con la contaminaci\\u00f3n ambiental y con tanta madre que tienen las vacunas, ya son un coctel peligroso. \\u00a1Pero ahora s\\u00famenle el virus! Todo eso ha ocasionado toda serie de reacciones, y debemos de estar preparados para una ola enorme de superh\\u00e9roes y supervillanos\\u201d.\\n\\nPor lo pronto, ya hemos visto las primeras acciones del Doctor Chaka, que ya no se sube a robar a los peseros, sino que se los lleva completos, mientras que el Capit\\u00e1n Canasta ha detenido a m\\u00e1s de un malandro arroj\\u00e1ndoles tacos de canasta cargados con energ\\u00eda radioactiva que los hace explosivos. Y es s\\u00f3lo el comienzo: \\u00abOlv\\u00eddense de las pel\\u00edculas de los Avengers, porque lo que ver\\u00e1 en esta ciudad los va a dejar a todos bien pen\\u2026\\u00bb, finaliz\\u00f3.\",\n          \"\\u00bfCre\\u00f3 un sismo artificial la celebraci\\u00f3n del gol de M\\u00e9xico frente a Alemania?. (CNN) \\u2013 Un tuit que describ\\u00eda un terremoto artificial en la Ciudad de M\\u00e9xico, despu\\u00e9s de que M\\u00e9xico anotara lo que ser\\u00eda el gol ganador del partido en la Copa del Mundo contra Alemania, se volvi\\u00f3 viral el domingo. Pero ni el Servicio Geol\\u00f3gico de los EE.UU. ni el Servicio Geol\\u00f3gico Nacional de M\\u00e9xico informaron de un sismo en la Ciudad de M\\u00e9xico ese d\\u00eda.\\n\\nEntonces, \\u00bfqu\\u00e9 pas\\u00f3? \\u00bfCelebr\\u00f3 M\\u00e9xico tan fuerte que caus\\u00f3 un sismo artificial?\\n\\nEl evento no fue lo suficientemente grande como para medirse en magnitudes y no habr\\u00eda sido perceptible para la poblaci\\u00f3n en general, de acuerdo con el Instituto de Investigaciones Geol\\u00f3gicas y Atmosf\\u00e9ricas, que no es una agencia del gobierno.\\n\\nMIRA: La Ciudad de M\\u00e9xico celebra el triunfo del \\u201cTri\\u201d\\n\\nEl domingo, el instituto tuite\\u00f3 lecturas sismogr\\u00e1ficas que destacaban la actividad cuando los mexicanos celebraron el que ser\\u00eda el gol decisivo del delantero Hirving Lozano.\\n\\nAtribuy\\u00f3 la causa posiblemente a \\u201csaltos masivos\\u201d de celebraci\\u00f3n en una publicaci\\u00f3n que obtuvo m\\u00e1s de 27.000 retuits.\\n\\nAl menos dos de sus sensores dentro de la Ciudad de M\\u00e9xico detectaron un movimiento s\\u00edsmico durante el partido de la Copa Mundial, \\u201cprobablemente producido por la celebraci\\u00f3n masiva\\u201d, seg\\u00fan la publicaci\\u00f3n del blog del instituto.\\n\\nMIRA: Hirving Lozano, el h\\u00e9roe de la victoria hist\\u00f3rica de M\\u00e9xico ante Alemania\\n\\nDijo que \\u201ctales eventos no son muy grandes. Solo los equipos sismogr\\u00e1ficos sensibles (y generalmente cercanos) pueden detectar los efectos de las multitudes\\u201d.\\n\\nEntonces, los instrumentos m\\u00e1s cercanos, por lo tanto, solo un \\u201cmuy peque\\u00f1o n\\u00famero de sism\\u00f3grafos\\u201d, pueden medir el evento, que el instituto describi\\u00f3 como \\u201cmicroregistros\\u201d.\\n\\nLa publicaci\\u00f3n del blog tambi\\u00e9n se\\u00f1al\\u00f3 que un evento similar ocurri\\u00f3 durante un juego de la NFL 2011 cuando una anotaci\\u00f3n de Marshawn Lynch provoc\\u00f3 que los fan\\u00e1ticos de Seattle Seahawks estallaran en celebraci\\u00f3n y eso caus\\u00f3 que un sism\\u00f3metro cercano midiera las vibraciones en lo que se llam\\u00f3 \\u201cQuaast de la Bestia\\u201d.\\n\\nLos fan\\u00e1ticos de Seattle volvieron a sacudir las cosas en 2013, registr\\u00e1ndose en una estaci\\u00f3n de grabaci\\u00f3n sismol\\u00f3gica cerca del estadio de los Seahawks durante una victoria sobre los New Orleans Saints.\\n\\nSismo o no, el entusiasmo en M\\u00e9xico se sinti\\u00f3 por todos lados el domingo cuando los fan\\u00e1ticos se deleitaban con el malestar general sobre Alemania.\\n\\nMarilia Brocchetto y Flora Charner de CNN contribuyeron a este informe.\",\n          \"Pedro S\\u00e1nchez busca reformar econom\\u00eda en Espa\\u00f1a tras covid-19. El presidente de Espa\\u00f1a, Pedro S\\u00e1nchez, compareci\\u00f3 hoy en el Palacio de la Moncloa para informar del Plan de Recuperaci\\u00f3n, Transformaci\\u00f3n y Resiliencia de la econom\\u00eda que pr\\u00f3ximamente presentar\\u00e1 a la Uni\\u00f3n Europea (UE), el cual trata el proyecto m\\u00e1s ambicioso e importante de la historia reciente del pa\\u00eds, seg\\u00fan el mandatario, tras su ingreso en dicho organismo.\\n\\n\\u201cEl Plan de Recuperaci\\u00f3n es el plan econ\\u00f3mico m\\u00e1s ambicioso y trascendental de las recientes historia econ\\u00f3mica para Espa\\u00f1a. Es la mayor oportunidad para Espa\\u00f1a desde la entrada de Espa\\u00f1a en la UE, y de eso hace 37 a\\u00f1os. Ocasiones como \\u00e9sta se presentan un par de veces en el siglo y esta oportunidad Espa\\u00f1a no la va a dejar pasar\\u201d, declar\\u00f3 S\\u00e1nchez.\\n\\nRecord\\u00f3 que en aquella \\u00e9poca los fondos estructurales fueron 8 mil millones de euros (m\\u00e1s de 191 mil millones de pesos) y \\u201cahora hablamos de 140 mil millones (superior a los 3 mil billones de pesos), fundamentales para recuperar los niveles de PIB previos a la pandemia y dar un crecimiento exponencial en las posibilidades de crecer, crear empleos y empresas. Una inversi\\u00f3n absolutamente excepcional y \\u00fanica\\u201d.\\n\\nEl mandatario dijo que en los \\u00faltimos a\\u00f1os Espa\\u00f1a ha sido golpeada por la peor crisis econ\\u00f3mica en 80 a\\u00f1os y un calificativo que tambi\\u00e9n consider\\u00f3 por la actual pandemia en un siglo. Estos sucesos desvelaron fragilidades de la econom\\u00eda y los servicios p\\u00fablicos del pa\\u00eds, seg\\u00fan S\\u00e1nchez.\\n\\n\\u00bfCu\\u00e1les son los objetivos del plan econ\\u00f3mico de Espa\\u00f1a?\\n\\nLos cinco grandes objetivos del Plan que S\\u00e1nchez resumi\\u00f3 son:\\n\\nModernizar el tejido productivo , incluyendo la administraci\\u00f3n p\\u00fablica.\\n\\n, incluyendo la administraci\\u00f3n p\\u00fablica. Aumentar la productividad , uno de los principales talones de Aquiles que explican por qu\\u00e9 hay un renta per c\\u00e1pita m\\u00e1s baja que el resto de potencias europeas.\\n\\nImpulsar la capacidad de crear empleo de calidad en todo el territorio, luchando contra el desempleo estructural, de j\\u00f3venes y de g\\u00e9nero.\\n\\nReducir las brechas sociales y de g\\u00e9nero , ampliadas por la crisis y la emergencia sanitaria\\n\\nImpulsar una transformaci\\u00f3n medioambiental.\\n\\n\\u201cEn el corto plazo queremos promover la recuperaci\\u00f3n econ\\u00f3mica tras la emergencia sanitaria\\u201d, dijo y agreg\\u00f3 que el fin \\u201ca medio plazo es mejorar la productividad de la econom\\u00eda espa\\u00f1ola con la transici\\u00f3n ecol\\u00f3gica y la transformaci\\u00f3n digital\\u201d.\\n\\nAgreg\\u00f3 que \\u201cen el largo plazo lo que queremos es que Espa\\u00f1a tenga un crecimiento robusto econ\\u00f3mico, sostenible en lo fiscal y resiliente en lo medioambiental\\u201d.\\n\\n\\u00bfCu\\u00e1les son los inversores principales?\\n\\nEl mandatario tambi\\u00e9n explic\\u00f3 que hay un total de 20 inversiones principales para los pr\\u00f3ximos tres a\\u00f1os. Detall\\u00f3 que en primer lugar habr\\u00e1 una estrategia de movilidad sostenible, segura y conectada, electrificando v\\u00edas p\\u00fablicas, dando puntos de recarga a veh\\u00edculos el\\u00e9ctricos, por 13 mil 200 millones de euros.\\n\\nEn la modernizaci\\u00f3n de la administraci\\u00f3n p\\u00fablica ser\\u00e1 de 4 mil 315 millones, mientras que en el plan de digitalizaci\\u00f3n de las pymes (peque\\u00f1as y medianas empresas) se planea con 4 mil 60 millones; la hoja de ruta del 5G, para impulsar la digitalizaci\\u00f3n, con 4 mil millones de euros; una nueva pol\\u00edtica industrial para la Espa\\u00f1a de 2030, con remodelaci\\u00f3n de gesti\\u00f3n de residuos, con 3 mil 780 millones.\\n\\nTambi\\u00e9n se contempla un plan de formaci\\u00f3n digital, con 3 mil 590 millones de euros; para la mejora de la competitividad se considera 3 mil 400 millones; el desarrollo de sistema de ciencia e innovaci\\u00f3n, con 3 mil 380 millones de euros; se tomar\\u00e1n 3 mil 400 millones de euros al sector tur\\u00edstico y, por \\u00faltimo, se destinar\\u00e1n 3 mil 175 millones a la transici\\u00f3n medioambiental.\\n\\nAsimismo, S\\u00e1nchez dijo que hay cuatro grandes transformaciones que consider\\u00f3 imprescindibles: la transici\\u00f3n verde, la transformaci\\u00f3n digital, la cohesi\\u00f3n territorial y la igualdad de g\\u00e9nero.\\n\\nLAS REFORMAS\\n\\nSon 20 las principales reformas que contempla el plan de Recuperaci\\u00f3n, aunque hay un centenar. No obstante, el presidente destac\\u00f3 la modernizaci\\u00f3n del sistema nacional de salud; un nuevo sistema energ\\u00e9tico, con despliegue de energ\\u00edas renovables; la modernizaci\\u00f3n de la justicia; la nueva econom\\u00eda de los cuidados, incluyendo las lecciones que deja la pandemia en las residencias; un plan de depuraci\\u00f3n y reutilizaci\\u00f3n de aguas, un debate clave en islas y territorios agr\\u00edcolas ante la amenaza del cambio clim\\u00e1tico.\\n\\nAdem\\u00e1s, a\\u00f1adi\\u00f3 la modernizaci\\u00f3n y digitalizaci\\u00f3n de administraciones p\\u00fablicas; la pol\\u00edtica de residuos y el impulso a la econom\\u00eda circular, que permitir\\u00e1n mucho empleo de calidad en los pr\\u00f3ximos a\\u00f1os; econom\\u00eda sostenible y conectada; la apuesta por la innovaci\\u00f3n y una nueva pol\\u00edtica de vivienda \\u201cen toda su extensi\\u00f3n\\u201d.\\n\\nLas previsiones del gobierno espa\\u00f1ol se traducir\\u00e1n, seg\\u00fan Pedro S\\u00e1nchez, en un crecimiento adicional del PIB de dos puntos a partir de 2021 (con crecimientos superiores al 2 por ciento anual desde 2030; la creaci\\u00f3n de 800 mil puestos de trabajo y la mejora de la vertebraci\\u00f3n del pa\\u00eds y el reparto de poblaci\\u00f3n en todo el pa\\u00eds, haciendo frente al reto demogr\\u00e1fico.\\n\\nEl env\\u00edo del plan definitivo a la UE, que inicialmente se preve\\u00eda entregar a finales de marzo, se retrasar\\u00e1 finalmente hasta su aprobaci\\u00f3n en el Consejo de Ministros del pr\\u00f3ximo martes 20 o 27 de abril.\\n\\n\\n\\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0,\n          1,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "dataset.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Creaci√≥n de modelo"
      ],
      "metadata": {
        "id": "BjXa7M6-D0q9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9YcLFvdxG5p"
      },
      "source": [
        "## 2.1 Inicializaci√≥n del modelo BETO\n",
        "\n",
        "Se carga el modelo `dccuchile/bert-base-spanish-wwm-cased`, una versi√≥n de BERT entrenada en corpus en espa√±ol con *Whole Word Masking* (WWM), lo que permite capturar mejor las propiedades l√©xicas y sint√°cticas del idioma.\n",
        "\n",
        "Tambi√©n se carga el tokenizador correspondiente, que convierte cada texto en una secuencia de tokens con padding y truncamiento adecuados. Esta etapa garantiza que la entrada al modelo sea coherente y consistente con el preentrenamiento original.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "hkJlMsZAwuUm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd5425c6-3a14-403c-968a-464acc45580c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertModel were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"dccuchile/bert-base-spanish-wwm-cased\")\n",
        "beto_model = AutoModel.from_pretrained(\"dccuchile/bert-base-spanish-wwm-cased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Definici√≥n de arquitectura de clasificaci√≥n (BETO + MLP)\n",
        "\n",
        "La arquitectura del modelo se compone de dos partes:\n",
        "\n",
        "1. **BETO** act√∫a como extractor de caracter√≠sticas (encoder), generando embeddings para cada token del texto.\n",
        "2. **Cabeza de clasificaci√≥n (MLP):**\n",
        "   - Se extrae el vector `[CLS]` como representaci√≥n del texto completo.\n",
        "   - Se pasa por dos capas densas:\n",
        "     - `Linear(768 ‚Üí 256)` con activaci√≥n ReLU y Dropout.\n",
        "     - `Linear(256 ‚Üí 2)` para salida binaria (real o falsa).\n",
        "\n",
        "> Esta estructura forma un **MLP (Multilayer Perceptron)** superficial que aprende a clasificar el embedding global en una de las dos clases.\n",
        "\n",
        "El modelo se entrena con:\n",
        "- `CrossEntropyLoss`: para clasificaci√≥n multiclase.\n",
        "- `AdamW`: optimizador adaptado con regularizaci√≥n L2.\n",
        "\n",
        "La arquitectura se mueve a GPU si est√° disponible.\n"
      ],
      "metadata": {
        "id": "JeKsJU5PEB8n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "jupeLlLpxUny",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a50be88-4564-4f46-9d5f-714c0db37888"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelo listo. Capas adicionales:\n",
            " Linear(in_features=768, out_features=256, bias=True) Linear(in_features=256, out_features=3, bias=True)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Definici√≥n del modelo\n",
        "dropout = nn.Dropout(0.3)\n",
        "dense1 = nn.Linear(beto_model.config.hidden_size, 256)\n",
        "dense2 = nn.Linear(256, 3)  # 2 clases (softmax impl√≠cito en la p√©rdida)\n",
        "\n",
        "# Mover a GPU\n",
        "beto_model.to(device)\n",
        "dropout.to(device)\n",
        "dense1.to(device)\n",
        "dense2.to(device)\n",
        "\n",
        "# Definici√≥n de funci√≥n forward\n",
        "def model_forward(input_ids, attention_mask):\n",
        "    outputs = beto_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    cls_output = outputs.last_hidden_state[:, 0, :]  # [CLS] token\n",
        "\n",
        "    x = dropout(cls_output)\n",
        "    x = F.relu(dense1(x))\n",
        "    x = dropout(x)\n",
        "    logits = dense2(x)\n",
        "\n",
        "    return logits\n",
        "\n",
        "# P√©rdida y optimizador\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(\n",
        "    list(beto_model.parameters()) + list(dense1.parameters()) + list(dense2.parameters()),\n",
        "    lr=2e-5\n",
        ")\n",
        "\n",
        "# Mostrar resumen b√°sico\n",
        "print(\"Modelo listo. Capas adicionales:\\n\", dense1, dense2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 Divisi√≥n del dataset y tokenizaci√≥n\n",
        "\n",
        "Se divide el conjunto de datos en entrenamiento (80%) y validaci√≥n (20%) usando `train_test_split` estratificado, para preservar la proporci√≥n entre clases reales y falsas.\n",
        "\n",
        "Luego, los textos se tokenizan utilizando el tokenizador de BETO:\n",
        "- Truncamiento a m√°ximo 512 tokens.\n",
        "- Padding autom√°tico.\n",
        "- Salida como tensores PyTorch (`return_tensors=\"pt\"`).\n",
        "\n",
        "Esta representaci√≥n ser√° utilizada como entrada al modelo en la etapa de entrenamiento. La tokenizaci√≥n garantiza que el texto est√© alineado con el vocabulario y segmentaci√≥n aprendidos durante el preentrenamiento de BETO.\n"
      ],
      "metadata": {
        "id": "wBrOIMqkEQ2g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "r2lXeYeoNyXF"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split data\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    df['text'], df['label'], test_size=0.2, random_state=42, stratify=df['label']\n",
        ")\n",
        "\n",
        "# Flatten labels\n",
        "train_labels = train_labels.values.flatten()\n",
        "val_labels = val_labels.values.flatten()\n",
        "\n",
        "# Tokenize text con salida en PyTorch\n",
        "train_encodings = tokenizer(\n",
        "    list(train_texts),\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    max_length=512,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "val_encodings = tokenizer(\n",
        "    list(val_texts),\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    max_length=512,\n",
        "    return_tensors=\"pt\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7rjqmfANvDQ"
      },
      "source": [
        "## 3. Entrenamiento del modelo textual (BETO + MLP)\n",
        "\n",
        "### üîß Preparaci√≥n\n",
        "- Se entrena el modelo por un m√°ximo de **20 √©pocas** con un `batch size` de 8.\n",
        "- Se aplica **regularizaci√≥n L2** (`weight_decay=1e-4`) y **Dropout del 30%** para mitigar el sobreajuste.\n",
        "- Se calculan los pesos por clase con `class_weight='balanced'` para compensar el posible desbalance entre noticias reales y falsas.\n",
        "- Se utiliza `CrossEntropyLoss` ponderada como funci√≥n de p√©rdida y `Adam` como optimizador.\n",
        "\n",
        "### 3.1 Bucle de entrenamiento\n",
        "Por cada √©poca:\n",
        "1. Se realiza entrenamiento en el conjunto de entrenamiento y evaluaci√≥n en el de validaci√≥n.\n",
        "2. Se almacenan m√©tricas clave: p√©rdida (`loss`) y precisi√≥n (`accuracy`) en ambos conjuntos.\n",
        "3. Se imprime el desempe√±o actual incluyendo la tasa de aprendizaje.\n",
        "4. Se aplica `ReduceLROnPlateau` como **scheduler din√°mico** que reduce la tasa de aprendizaje si la p√©rdida de validaci√≥n no mejora.\n",
        "5. Se implementa **early stopping** con `patience = 3`, deteniendo el entrenamiento si no se detecta mejora.\n",
        "\n",
        "### 3.2 Restauraci√≥n del mejor modelo\n",
        "Al finalizar el entrenamiento (o al activarse early stopping), se restauran los pesos del modelo con mejor desempe√±o en validaci√≥n, preservando as√≠ el mejor punto del entrenamiento.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51xyCJgMPo8u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7af608d2-27cf-4dfc-d683-2b276a9aa15c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | LR: 0.000020 | Train Loss: 0.9162 | Val Loss: 0.7242 | Train Acc: 0.5395 | Val Acc: 0.6385\n",
            "Epoch 2 | LR: 0.000020 | Train Loss: 0.5736 | Val Loss: 0.5397 | Train Acc: 0.7601 | Val Acc: 0.7962\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.utils import class_weight\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "EPOCHS = 20\n",
        "BATCH_SIZE = 8\n",
        "PATIENCE = 3\n",
        "\n",
        "dropout = torch.nn.Dropout(0.3)  # Increased regularization\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(beto_model.parameters()) + list(dense1.parameters()) + list(dense2.parameters()),\n",
        "    lr=2e-5,\n",
        "    weight_decay=1e-4\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# Compute class weights\n",
        "class_weights = class_weight.compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(train_labels),\n",
        "    y=train_labels\n",
        ")\n",
        "\n",
        "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "criterion = torch.nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_dataset = TensorDataset(\n",
        "    train_encodings['input_ids'],\n",
        "    train_encodings['attention_mask'],\n",
        "    torch.tensor(train_labels)\n",
        ")\n",
        "\n",
        "val_dataset = TensorDataset(\n",
        "    val_encodings['input_ids'],\n",
        "    val_encodings['attention_mask'],\n",
        "    torch.tensor(val_labels)\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "# Scheduler\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1)\n",
        "\n",
        "# Metrics\n",
        "train_losses, val_losses = [], []\n",
        "train_accuracies, val_accuracies = [], []\n",
        "\n",
        "best_val_loss = np.inf\n",
        "best_model_state = None\n",
        "patience_counter = 0\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(EPOCHS):\n",
        "    beto_model.train()\n",
        "    dense1.train()\n",
        "    dense2.train()\n",
        "\n",
        "    running_loss = 0\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        input_ids, attention_mask, labels = [x.to(device) for x in batch]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model_forward(input_ids, attention_mask)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    avg_train_loss = running_loss / len(train_loader)\n",
        "    train_acc = correct / total\n",
        "\n",
        "    # Validation\n",
        "    beto_model.eval()\n",
        "    dense1.eval()\n",
        "    dense2.eval()\n",
        "\n",
        "    val_running_loss = 0\n",
        "    val_correct, val_total = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids, attention_mask, labels = [x.to(device) for x in batch]\n",
        "            outputs = model_forward(input_ids, attention_mask)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_running_loss += loss.item()\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            val_correct += (preds == labels).sum().item()\n",
        "            val_total += labels.size(0)\n",
        "\n",
        "    avg_val_loss = val_running_loss / len(val_loader)\n",
        "    val_acc = val_correct / val_total\n",
        "\n",
        "    # Save metrics\n",
        "    train_losses.append(avg_train_loss)\n",
        "    val_losses.append(avg_val_loss)\n",
        "    train_accuracies.append(train_acc)\n",
        "    val_accuracies.append(val_acc)\n",
        "\n",
        "    # Print status\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "    print(f\"Epoch {epoch+1} | LR: {current_lr:.6f} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | \"\n",
        "          f\"Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    # Scheduler step\n",
        "    scheduler.step(avg_val_loss)\n",
        "\n",
        "    # Early stopping\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        best_model_state = {\n",
        "            \"beto\": beto_model.state_dict(),\n",
        "            \"dense1\": dense1.state_dict(),\n",
        "            \"dense2\": dense2.state_dict()\n",
        "        }\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= PATIENCE:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "# Restore best model\n",
        "beto_model.load_state_dict(best_model_state[\"beto\"])\n",
        "dense1.load_state_dict(best_model_state[\"dense1\"])\n",
        "dense2.load_state_dict(best_model_state[\"dense2\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Visualizaci√≥n de m√©tricas y evaluaci√≥n del modelo textual\n",
        "\n",
        "### 4.1 Precisi√≥n y p√©rdida a lo largo de las √©pocas\n",
        "\n",
        "Se grafican las curvas de `accuracy` y `loss` tanto para el conjunto de entrenamiento como para el de validaci√≥n.\n",
        "\n",
        "#### 4.2 Observaciones:\n",
        "- La precisi√≥n en entrenamiento alcanza casi el 100%, mientras que en validaci√≥n se estabiliza cerca del 86%.\n",
        "- La p√©rdida de entrenamiento disminuye continuamente, pero la de validaci√≥n comienza a subir despu√©s de la √©poca 2.\n",
        "\n",
        "> Estos patrones son consistentes con **sobreajuste**, indicando que el modelo aprende muy bien el set de entrenamiento pero generaliza moderadamente al set de validaci√≥n.\n",
        "\n"
      ],
      "metadata": {
        "id": "L_dlWLpC-MGv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRoJ25UZQlnC"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Accuracy\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(train_accuracies, label='Training Accuracy')\n",
        "plt.plot(val_accuracies, label='Validation Accuracy')\n",
        "plt.title('Accuracy over epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Loss\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.title('Loss over epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3 Evaluaci√≥n cuantitativa (classification report)\n",
        "\n",
        "Se calcula un reporte detallado con precisi√≥n, recall y F1-score por clase.\n",
        "\n",
        "- **Accuracy global:** 0.86\n",
        "- **Macro F1:** 0.86\n",
        "\n",
        "#### 4.3.1 An√°lisis:\n",
        "- El modelo **identifica muy bien noticias reales** (recall del 94%), pero **omite una proporci√≥n considerable de noticias falsas** (recall del 77%).\n",
        "- Esto sugiere que el modelo confunde algunas noticias falsas con verdaderas, posiblemente debido a redacci√≥n formal o ambig√ºedad sem√°ntica.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aZfyJ_6fJWSE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gU8xiGmQ5gC"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score, classification_report\n",
        "import torch\n",
        "\n",
        "# Modo evaluaci√≥n\n",
        "beto_model.eval()\n",
        "dense1.eval()\n",
        "dense2.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in val_loader:\n",
        "        input_ids, attention_mask, labels = [x.to(device) for x in batch]\n",
        "\n",
        "        outputs = model_forward(input_ids, attention_mask)\n",
        "        preds = outputs.argmax(dim=1)\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Reporte de m√©tricas\n",
        "print(classification_report(all_labels, all_preds, target_names=[\"Real\", \"Fake\", \"Satira\"]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.4 Matriz de confusi√≥n\n",
        "\n",
        "La matriz muestra los errores m√°s relevantes del modelo en validaci√≥n.\n",
        "\n",
        "- **Falsos positivos:** 8 reales etiquetadas como falsas.\n",
        "- **Falsos negativos:** 23 falsas clasificadas como reales.\n",
        "\n",
        "> El mayor error est√° en las **falsas negativas**, lo que impacta negativamente el recall de la clase ‚ÄúFake‚Äù.\n"
      ],
      "metadata": {
        "id": "yaiaoTnoJv8O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jErQuyR9Q8kY"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Ya tienes all_preds y all_labels del paso anterior\n",
        "\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "labels = [\"Real\", \"Fake\", \"Satira\"]\n",
        "\n",
        "# Mostrar matriz de confusi√≥n\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.5 Conclusi√≥n\n",
        "\n",
        "- El modelo logra un buen desempe√±o general como l√≠nea base, con un F1 de 0.86 y buena estabilidad.\n",
        "- El sobreajuste observado y los errores en noticias falsas justifican continuar con la incorporaci√≥n de contexto estructural (GNN) y t√©cnicas de regularizaci√≥n o augmentaci√≥n."
      ],
      "metadata": {
        "id": "948n8P24J6u7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0U7uadgJ9_90"
      },
      "source": [
        "## 5. Creaci√≥n del modelo h√≠brido"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtR5maZQHOIV"
      },
      "source": [
        "### 5.1 Construcci√≥n de features estructurales para la GNN\n",
        "\n",
        "En esta secci√≥n se construye un conjunto de **metadatos enriquecidos** que representan cada noticia como un nodo con atributos num√©ricos. Estos atributos no se basan en el contenido textual en s√≠, sino en se√±ales de contexto y estructura, lo que permite entrenar una red neuronal de grafos (GNN) que capte relaciones impl√≠citas entre noticias.\n",
        "\n",
        "---\n",
        "\n",
        "### 5.1.1 Atributos incluidos\n",
        "\n",
        "1. **`LABEL`**  \n",
        "   Etiqueta binaria de veracidad (0 = real, 1 = falsa), igual que en el modelo textual.\n",
        "\n",
        "2. **`FUENTE_COD`**  \n",
        "   Codificaci√≥n del autor o fuente de la noticia. Se usa `LabelEncoder` para transformar valores categ√≥ricos a enteros. Las fuentes desconocidas se rellenan con `\"desconocido\"`.\n",
        "\n",
        "3. **`FECHA_NUM`**  \n",
        "   Representaci√≥n de la fecha de publicaci√≥n como n√∫mero de d√≠as desde una fecha base (`2023-01-01`). Esto permite tratar la fecha como una variable continua.\n",
        "\n",
        "4. **`LONGITUD`**  \n",
        "   Cantidad de palabras del texto completo (`t√≠tulo + cuerpo`). Las noticias extremadamente cortas o largas pueden correlacionarse con ciertos tipos de desinformaci√≥n.\n",
        "\n",
        "5. **`POLARIDAD`**  \n",
        "   Sentimiento del texto calculado con `TextBlob` (rango de -1 a 1). Las noticias con polarizaci√≥n extrema podr√≠an ser m√°s susceptibles a contener sesgos o emociones.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "3XwgVeVTtN2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DNGvccXWHQdu"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Crear dataset limpio\n",
        "gnn_data = pd.DataFrame()\n",
        "\n",
        "# Texto y label\n",
        "gnn_data['TEXTO'] = df['TITULO'].fillna('').astype(str) + \". \" + df['CORPUS'].fillna('').astype(str)\n",
        "gnn_data['LABEL'] = df['VERACIDAD'].map({'true': 0, 'false': 1, \"satira\": 2})\n",
        "\n",
        "# Fuente / Dominio\n",
        "gnn_data['FUENTE'] = df['AUTOR'].fillna('desconocido').astype(str)\n",
        "le_fuente = LabelEncoder()\n",
        "gnn_data['FUENTE_COD'] = le_fuente.fit_transform(gnn_data['FUENTE'])\n",
        "\n",
        "# Fecha a n√∫mero (d√≠as desde 2023-01-01)\n",
        "fecha_col = pd.to_datetime(df['FECHA'], errors='coerce')\n",
        "fecha_base = pd.Timestamp('2023-01-01')\n",
        "# Serie de d√≠as (puede contener NaN)\n",
        "fecha_num = (fecha_col - fecha_base).dt.days\n",
        "# Imputaci√≥n simple (si todas son NaN ‚Üí 0)\n",
        "if fecha_num.notna().any():\n",
        "    fecha_num = fecha_num.fillna(fecha_num.median())\n",
        "else:\n",
        "    fecha_num = pd.Series(0, index=df.index, dtype=float)\n",
        "\n",
        "gnn_data['FECHA_NUM'] = fecha_num.astype(float)\n",
        "\n",
        "# Longitud del texto\n",
        "gnn_data['LONGITUD'] = gnn_data['TEXTO'].str.split().str.len()\n",
        "\n",
        "# Polaridad\n",
        "def get_sentiment(text):\n",
        "    try:\n",
        "        return TextBlob(str(text)).sentiment.polarity\n",
        "    except Exception:\n",
        "        return 0.0\n",
        "\n",
        "gnn_data['POLARIDAD'] = gnn_data['TEXTO'].apply(get_sentiment)\n",
        "\n",
        "gnn_data.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5.1.2 Detecci√≥n de Clickbait\n",
        "\n",
        "Se utiliza el modelo `taniwasl/clickbait_es` (HuggingFace) para calcular un **score de clickbait** a partir del t√≠tulo de la noticia.\n",
        "\n",
        "- Se define un pipeline con `TextClassificationPipeline` que devuelve todas las probabilidades por clase.\n",
        "- Se extrae la probabilidad correspondiente a la clase ‚Äúclickbait‚Äù.\n",
        "- El resultado es un valor continuo entre 0 y 1 asignado a la columna `CLICKBAIT`.\n",
        "\n",
        "> Este puntaje refleja la probabilidad de que el t√≠tulo de una noticia tenga intenci√≥n de atraer clics mediante lenguaje exagerado, sensacionalista o ambiguo.\n"
      ],
      "metadata": {
        "id": "0glsr0eYLICP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJeWPFmjMwWU"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline\n",
        "\n",
        "clickbait_model = \"taniwasl/clickbait_es\"\n",
        "\n",
        "tokenizer_cb = AutoTokenizer.from_pretrained(clickbait_model)\n",
        "model_cb = AutoModelForSequenceClassification.from_pretrained(clickbait_model)\n",
        "\n",
        "cb_pipeline = TextClassificationPipeline(\n",
        "    model=model_cb,\n",
        "    tokenizer=tokenizer_cb,\n",
        "    return_all_scores=True,\n",
        "    function_to_apply=\"softmax\",\n",
        "    top_k=None\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1.3 Resultado\n",
        "\n",
        "El dataframe `gnn_data` contiene ahora las siguientes variables por nodo:\n",
        "\n",
        "- TEXTO (para referencia)\n",
        "- LABEL (0 o 1)\n",
        "- FUENTE_COD (entero)\n",
        "- FECHA_NUM (entero)\n",
        "- LONGITUD (entero)\n",
        "- POLARIDAD (float)\n",
        "- CLICKBAIT (float)\n",
        "\n",
        "Este vector de caracter√≠sticas representa el perfil contextual de cada noticia y ser√° utilizado como entrada para construir un grafo de similitud y entrenar el modelo GNN."
      ],
      "metadata": {
        "id": "-sRmnPVeLP6s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YcwjqI5BN8Bd"
      },
      "outputs": [],
      "source": [
        "def clickbait_score(title: str) -> float:\n",
        "    res = cb_pipeline(title[:128])\n",
        "    for score_dict in res[0]:\n",
        "        if score_dict['label'].lower() in ['clickbait', 'click_bait', 'click-bait']:\n",
        "            return score_dict['score']\n",
        "    return res[0][0]['score']\n",
        "\n",
        "gnn_data['CLICKBAIT'] = df['TITULO'].apply(clickbait_score)\n",
        "\n",
        "gnn_data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1.4 Normalizaci√≥n de metadatos y ensamblado del vector estructural\n",
        "\n",
        "Para que la red neuronal de grafos procese correctamente los metadatos, es necesario normalizar las variables num√©ricas a un rango comparable. Se utiliza `MinMaxScaler` para escalar los siguientes atributos al intervalo [0, 1]:\n",
        "\n",
        "- `FECHA_NUM` ‚Üí `FECHA_NORM`\n",
        "- `LONGITUD` ‚Üí `LONGITUD_NORM`\n",
        "- `POLARIDAD` ‚Üí `POLARIDAD_NORM`\n",
        "- `CLICKBAIT` ‚Üí `CLICKBAIT_NORM`\n",
        "\n",
        "Luego, se construye la **matriz final de metadatos** (`metadata_gnn`) concatenando:\n",
        "\n",
        "1. `FUENTE_COD` ‚Üí codificaci√≥n entera de la fuente (no normalizada).\n",
        "2. Atributos normalizados: fecha, longitud, polaridad y clickbait.\n",
        "\n",
        "Este vector representa cada nodo (noticia) en el grafo mediante un conjunto de **caracter√≠sticas estructurales y sem√°nticas suaves**, que ser√°n procesadas posteriormente por la GNN para capturar relaciones no evidentes entre noticias.\n",
        "\n",
        "> Esta representaci√≥n es independiente del contenido textual y permite explotar patrones relacionados con estilo, fuente, tono y temporalidad.\n"
      ],
      "metadata": {
        "id": "bjG9YMKaL0OT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTt_7m1JObqX"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# √çndices de train/test (estratificado en la etiqueta)\n",
        "idx_all = np.arange(len(gnn_data))\n",
        "train_idx, test_idx = train_test_split(\n",
        "    idx_all,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=gnn_data['LABEL']\n",
        ")\n",
        "\n",
        "# Features num√©ricas sin normalizar (orden expl√≠cito)\n",
        "# 1=LONGITUD, 2=POLARIDAD, 3=CLICKBAIT, 4=FECHA_NUM\n",
        "X_num = gnn_data[['LONGITUD', 'POLARIDAD', 'CLICKBAIT', 'FECHA_NUM']].to_numpy(dtype=float)\n",
        "\n",
        "# Split\n",
        "X_num_train = X_num[train_idx]\n",
        "X_num_test  = X_num[test_idx]\n",
        "\n",
        "# Escalado solo con TRAIN (evita fuga)\n",
        "scaler = MinMaxScaler().fit(X_num_train)\n",
        "X_num_train_scaled = scaler.transform(X_num_train)\n",
        "X_num_test_scaled  = scaler.transform(X_num_test)\n",
        "\n",
        "# Fuente codificada (queda como primera columna, SIN escalar)\n",
        "fuente_all = gnn_data['FUENTE_COD'].to_numpy().reshape(-1,1)\n",
        "fuente_train = fuente_all[train_idx]\n",
        "fuente_test  = fuente_all[test_idx]\n",
        "\n",
        "\n",
        "metadata_train = np.concatenate([fuente_train, X_num_train_scaled], axis=1).astype(np.float32)\n",
        "metadata_test  = np.concatenate([fuente_test , X_num_test_scaled ], axis=1).astype(np.float32)\n",
        "\n",
        "meta_train = metadata_train\n",
        "meta_test  = metadata_test\n",
        "\n",
        "\n",
        "metadata_gnn = np.concatenate([metadata_train, metadata_test], axis=0)\n",
        "\n",
        "print(\"metadata_train/test:\", metadata_train.shape, metadata_test.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Arquitectura Modelo H√≠brido"
      ],
      "metadata": {
        "id": "Hz4UkURUL7_q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1 Extracci√≥n de embeddings con BETO\n",
        "\n",
        "Se utiliza el modelo `beto_model` (ya entrenado o congelado) para generar representaciones vectoriales (`embeddings`) de cada noticia a partir del token `[CLS]`.\n",
        "\n",
        "Pasos:\n",
        "1. Se tokeniza todo el texto (`T√çTULO + CUERPO`) y se almacena en un `DataLoader`.\n",
        "2. Se desactiva el c√°lculo de gradientes (`torch.no_grad()`).\n",
        "3. Para cada batch, se extrae el embedding del token `[CLS]` y se almacena.\n",
        "4. Finalmente, se concatenan todos los vectores obtenidos (uno por noticia), obteniendo una matriz de embeddings de dimensi√≥n `(n_samples, 768)`.\n",
        "\n",
        "Estos vectores representan el **contenido textual** de cada noticia en un espacio sem√°ntico preentrenado.\n"
      ],
      "metadata": {
        "id": "48U9Kra9Mxuw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_embeddings(text_list, batch_size=8):\n",
        "    encodings = tokenizer(\n",
        "        list(text_list),\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=512,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    dataset = TensorDataset(encodings['input_ids'], encodings['attention_mask'])\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    beto_model.eval()\n",
        "    all_cls = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for input_ids_batch, attention_mask_batch in loader:\n",
        "            input_ids_batch = input_ids_batch.to(device)\n",
        "            attention_mask_batch = attention_mask_batch.to(device)\n",
        "\n",
        "            outputs = beto_model(input_ids=input_ids_batch, attention_mask=attention_mask_batch)\n",
        "            cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
        "            all_cls.append(cls_embeddings.cpu())\n",
        "\n",
        "    return torch.cat(all_cls, dim=0).numpy()"
      ],
      "metadata": {
        "id": "bVLqyNkGA4Zb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "labels = gnn_data['LABEL'].to_numpy()\n",
        "texts  = gnn_data['TEXTO'].to_numpy()\n",
        "\n",
        "texts_train = texts[train_idx]\n",
        "texts_test  = texts[test_idx]\n",
        "y_train = labels[train_idx]\n",
        "y_test  = labels[test_idx]\n",
        "\n",
        "# Usa tus funciones intactas\n",
        "X_text_train = generate_embeddings(texts_train)\n",
        "X_text_test  = generate_embeddings(texts_test)\n",
        "\n",
        "# Asegura dtype consistente\n",
        "X_meta_train = meta_train.astype(np.float32)\n",
        "X_meta_test  = meta_test.astype(np.float32)\n",
        "\n",
        "print(\"X_text_train/test:\", X_text_train.shape, X_text_test.shape)\n",
        "print(\"X_meta_train/test:\", X_meta_train.shape, X_meta_test.shape)\n"
      ],
      "metadata": {
        "id": "xRYma0vZAYeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FkSwb4WORSV5"
      },
      "outputs": [],
      "source": [
        "# from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# # Tokenizar todo pero sin pasar a tensor gigante\n",
        "# encodings = tokenizer(\n",
        "#     list(gnn_data['TEXTO']),\n",
        "#     truncation=True,\n",
        "#     padding=True,\n",
        "#     max_length=512,\n",
        "#     return_tensors=\"pt\"\n",
        "# )\n",
        "\n",
        "# # Crear DataLoader sin clases personalizadas\n",
        "# input_ids = encodings['input_ids']\n",
        "# attention_mask = encodings['attention_mask']\n",
        "\n",
        "# dataset = TensorDataset(input_ids, attention_mask)\n",
        "# loader = DataLoader(dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "# # Modo evaluaci√≥n\n",
        "# beto_model.eval()\n",
        "# all_embeddings = []\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     for batch in loader:\n",
        "#         input_ids_batch, attention_mask_batch = [x.to(device) for x in batch]\n",
        "#         outputs = beto_model(input_ids=input_ids_batch, attention_mask=attention_mask_batch)\n",
        "#         cls_batch = outputs.last_hidden_state[:, 0, :]  # CLS token\n",
        "#         all_embeddings.append(cls_batch.cpu())\n",
        "\n",
        "# # Concatenar todos los embeddings\n",
        "# embeddings = torch.cat(all_embeddings, dim=0).numpy()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.2 Divisi√≥n del dataset para modelo h√≠brido\n",
        "\n",
        "Se utiliza `train_test_split` para dividir el conjunto completo de:\n",
        "- Embeddings del texto (`X_text`)\n",
        "- Metadatos estructurales (`X_meta`)\n",
        "- Etiquetas (`y`)\n",
        "\n",
        "Esto permite evaluar de manera justa el modelo h√≠brido posterior, asegurando que las relaciones en el grafo tambi√©n est√©n divididas entre entrenamiento y prueba. El `random_state` se fija para reproducibilidad.\n"
      ],
      "metadata": {
        "id": "9lwpDYI8NicS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.3 Construcci√≥n del grafo de similitud sem√°ntica\n",
        "\n",
        "Se construyen dos grafos de adyacencia (uno para entrenamiento y otro para prueba) basados en la **similitud coseno** entre los embeddings de texto.\n",
        "\n",
        "- Se calcula la matriz de similitud coseno entre pares de noticias.\n",
        "- Se aplica un umbral de corte: si la similitud es > 0.8, se considera que hay una arista entre los nodos (noticias).\n",
        "- Se elimina la diagonal (auto-conexiones).\n",
        "- Se convierte la matriz densa a formato disperso (`scipy.sparse`) para eficiencia.\n",
        "\n",
        "> Este grafo captura relaciones impl√≠citas entre noticias con contenido textual altamente similar, incluso si no comparten fuente o fecha.\n"
      ],
      "metadata": {
        "id": "0u0qSYPKNoue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fuentes_train = meta_train[:, 0]\n",
        "fechas_train      = meta_train[:, 4]   # FECHA_NORM\n",
        "polaridades_train = meta_train[:, 2]\n"
      ],
      "metadata": {
        "id": "A5_3QOpToRx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwCCPLwsSXpb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "n = len(X_text_train)\n",
        "\n",
        "# Cosine similarity con umbral aleatorio por epoch\n",
        "sim_text = cosine_similarity(X_text_train)\n",
        "threshold = np.random.uniform(0.7, 0.85)\n",
        "adj_text_train = (sim_text > threshold).astype(np.float32)\n",
        "\n",
        "# Misma fuente\n",
        "adj_fuente_train = np.equal.outer(fuentes_train, fuentes_train).astype(np.float32)\n",
        "\n",
        "# Fecha cercana (diferencia ‚â§ 0.05 en escala normalizada)\n",
        "adj_fecha_train = (np.abs(fechas_train[:, None] - fechas_train[None, :]) <= 0.05).astype(np.float32)\n",
        "\n",
        "# Polaridad similar (diferencia ‚â§ 0.1)\n",
        "adj_polaridad_train = (np.abs(polaridades_train[:, None] - polaridades_train[None, :]) <= 0.1).astype(np.float32)\n",
        "\n",
        "# Combinar\n",
        "adj_enriched_train = np.clip(\n",
        "    adj_text_train + adj_fuente_train + adj_fecha_train + adj_polaridad_train,\n",
        "    0, 1\n",
        ")\n",
        "np.fill_diagonal(adj_enriched_train, 0)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fuentes_test = meta_test[:, 0]\n",
        "fechas_test      = meta_test[:, 4]     # FECHA_NORM\n",
        "polaridades_test = meta_test[:, 2]   # Suponiendo ya est√° normalizada\n",
        "\n",
        "# 1. Similitud coseno (texto)\n",
        "sim_text_test = cosine_similarity(X_text_test)\n",
        "adj_text_test = (sim_text_test > 0.8).astype(np.float32)\n",
        "\n",
        "# 2. Misma fuente\n",
        "adj_fuente_test = np.equal.outer(fuentes_test, fuentes_test).astype(np.float32)\n",
        "\n",
        "# 3. Fecha cercana (normalizada)\n",
        "adj_fecha_test = (np.abs(fechas_test[:, None] - fechas_test[None, :]) <= 0.05).astype(np.float32)\n",
        "\n",
        "# 4. Polaridad similar\n",
        "adj_polaridad_test = (np.abs(polaridades_test[:, None] - polaridades_test[None, :]) <= 0.1).astype(np.float32)\n",
        "\n",
        "# 5. Combinar todo (OR l√≥gico)\n",
        "adj_enriched_test = np.clip(\n",
        "    adj_text_test + adj_fuente_test + adj_fecha_test + adj_polaridad_test,\n",
        "    0, 1\n",
        ")\n",
        "np.fill_diagonal(adj_enriched_test, 0)\n"
      ],
      "metadata": {
        "id": "S47qWFidJMXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-geometric"
      ],
      "metadata": {
        "id": "5YwFR32YMIe1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.4 Definici√≥n del modelo h√≠brido (BETO + GCN)\n",
        "\n",
        "Se define una arquitectura que combina:\n",
        "- **Embeddings de texto (768 dim)** obtenidos por BETO.\n",
        "- **Metadatos procesados con GCN**, que modela la estructura del grafo.\n",
        "\n",
        "#### 6.4.1 Arquitectura:\n",
        "\n",
        "1. `meta_fc` (Linear): transforma los metadatos de entrada (5 caracter√≠sticas) a 64 dimensiones.\n",
        "2. `GCNConv`: aplica una convoluci√≥n de grafos que propaga informaci√≥n entre nodos vecinos en el grafo de similitud.\n",
        "3. `concat_fc1`: concatena las salidas del GCN y el embedding textual para formar un vector enriquecido de 800 dimensiones.\n",
        "4. `output_fc`: capa final con activaci√≥n `log_softmax` para clasificaci√≥n binaria.\n",
        "\n",
        "Se usan funciones de activaci√≥n ReLU y regularizaci√≥n con Dropout para mejorar la capacidad general del modelo.\n",
        "\n",
        "---\n",
        "\n",
        "#### 6.4.2 Ejecuci√≥n (Forward Pass)\n",
        "\n",
        "1. Los metadatos normalizados se transforman con `meta_fc` y luego se propagan en el grafo con `GCNConv`.\n",
        "2. El resultado del GCN se concatena con los embeddings del texto de BETO.\n",
        "3. Esta representaci√≥n combinada se pasa por capas densas y se obtiene el `logits` de clasificaci√≥n (dimensi√≥n 2).\n",
        "\n",
        "Este dise√±o permite capturar **simult√°neamente se√±ales ling√º√≠sticas y estructurales**, lo que mejora significativamente el desempe√±o frente a un modelo puramente textual.\n"
      ],
      "metadata": {
        "id": "4_bzv1v-NvTB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9WaZFuTsSanz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GATConv  # üîÑ Cambiado de GCNConv a GATConv\n",
        "from torch_geometric.utils import dense_to_sparse, dropout_adj\n",
        "\n",
        "# Dimensiones\n",
        "metadata_dim = X_meta_train.shape[1]\n",
        "text_dim = 768\n",
        "\n",
        "# Definir capas\n",
        "meta_fc = nn.Linear(metadata_dim, 32)\n",
        "gcn_conv = GATConv(32, 16, heads=2, concat=False, dropout=0.3)  # üîÑ GATConv con atenci√≥\n",
        "concat_fc1 = nn.Linear(text_dim + 16, 32)\n",
        "bn1 = nn.BatchNorm1d(32)\n",
        "dropout = nn.Dropout(0.5)\n",
        "output_fc = nn.Linear(32, 3)\n",
        "\n",
        "# Mover a GPU si est√° disponible\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "meta_fc.to(device)\n",
        "gcn_conv.to(device)\n",
        "concat_fc1.to(device)\n",
        "bn1.to(device)\n",
        "dropout.to(device)\n",
        "output_fc.to(device)\n",
        "\n",
        "# Preparar datos\n",
        "text_embeddings = torch.tensor(X_text_train, dtype=torch.float32).to(device)\n",
        "meta_features = torch.tensor(X_meta_train, dtype=torch.float32).to(device)\n",
        "\n",
        "# Convertir adj_train a edge_index\n",
        "adj_dense = torch.tensor(adj_enriched_train, dtype=torch.float32)\n",
        "edge_index, edge_weight = dense_to_sparse(adj_dense)\n",
        "edge_index = edge_index.to(device)\n",
        "\n",
        "# üîπ Aplicar edge dropout SOLO en entrenamiento\n",
        "# p=0.1 significa que elimina 10% de las aristas aleatoriamente\n",
        "is_training = True  # ponlo en False si es validaci√≥n o test\n",
        "if is_training:\n",
        "    edge_index, _ = dropout_adj(edge_index, p=0.1, force_undirected=False)\n",
        "    edge_index = edge_index.to(device)\n",
        "\n",
        "\n",
        "# Forward pass\n",
        "x_gnn = F.relu(meta_fc(meta_features))\n",
        "x_gnn = F.relu(gcn_conv(x_gnn, edge_index))  # üîÑ Ahora con atenci√≥n\n",
        "x_gnn = dropout(x_gnn)\n",
        "\n",
        "x_concat = torch.cat([text_embeddings, x_gnn], dim=1)\n",
        "x = F.relu(concat_fc1(x_concat))\n",
        "x = bn1(x)\n",
        "x = dropout(x)\n",
        "logits = F.log_softmax(output_fc(x), dim=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Entrenamiento del modelo h√≠brido BETO + GCN\n",
        "\n",
        "Se entrena la arquitectura que combina embeddings textuales (BETO) con metadatos procesados mediante una red neuronal de grafos (`GCNConv`).\n",
        "\n",
        "### 7.1 Configuraci√≥n\n",
        "- **Loss:** `NLLLoss` (por uso de `log_softmax`)\n",
        "- **Optimizador:** Adam\n",
        "- **√âpocas:** 20\n",
        "- **M√©tricas:** accuracy y p√©rdida en entrenamiento y validaci√≥n\n",
        "\n",
        "### 7.2 Proceso por √©poca\n",
        "1. Los metadatos se proyectan con `meta_fc` y se propagan en el grafo con `GCNConv`.\n",
        "2. Se concatenan con los embeddings de texto (`X_text_*`).\n",
        "3. La representaci√≥n combinada pasa por capas densas (`concat_fc1` + `output_fc`).\n",
        "4. Se calcula la p√©rdida y se actualizan los pesos (entrenamiento).\n",
        "5. En validaci√≥n, se replica el forward pass sin actualizar par√°metros.\n",
        "\n",
        "### 7.3 M√©tricas\n",
        "Se registran precisi√≥n y p√©rdida por √©poca para evaluar desempe√±o. El grafo se reconstruye din√°micamente en cada conjunto (`adj_train`, `adj_test`).\n",
        "\n",
        "> Este modelo aprende simult√°neamente contenido y contexto, lo que mejora la detecci√≥n de desinformaci√≥n m√°s all√° del an√°lisis puramente textual.\n"
      ],
      "metadata": {
        "id": "OH7Y7f7EOClm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOi91XduSeW_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch_geometric.utils import dense_to_sparse\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Tensors de train/test (NO recrearlos en cada epoch)\n",
        "x_meta_tr = torch.tensor(X_meta_train, dtype=torch.float32, device=device)\n",
        "x_meta_te = torch.tensor(X_meta_test,  dtype=torch.float32, device=device)\n",
        "x_txt_tr  = torch.tensor(X_text_train, dtype=torch.float32, device=device)\n",
        "x_txt_te  = torch.tensor(X_text_test,  dtype=torch.float32, device=device)\n",
        "\n",
        "y_tr = torch.tensor(y_train, dtype=torch.long, device=device)\n",
        "y_te = torch.tensor(y_test,  dtype=torch.long, device=device)\n",
        "\n",
        "# edge_index (fijo por split)\n",
        "edge_index_tr, _ = dense_to_sparse(torch.tensor(adj_enriched_train, dtype=torch.float32, device=device))\n",
        "edge_index_te, _ = dense_to_sparse(torch.tensor(adj_enriched_test,  dtype=torch.float32, device=device))\n",
        "\n",
        "\n",
        "# Criterio y optimizador\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.05)  # ‚Üê evita sobreconfianza\n",
        "optimizer = optim.Adam(\n",
        "    list(meta_fc.parameters()) +\n",
        "    list(gcn_conv.parameters()) +\n",
        "    list(concat_fc1.parameters()) +\n",
        "    list(output_fc.parameters()),\n",
        "    lr=1e-3,\n",
        "    weight_decay=1e-3                                  # ‚Üê regulariza (L2)\n",
        ")\n",
        "\n",
        "# Scheduler (reduce LR si val_loss no mejora)\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1)\n",
        "\n",
        "# ---------------------------\n",
        "# Loop con early stopping (por AUROC)\n",
        "# ---------------------------\n",
        "EPOCHS   = 20\n",
        "PATIENCE = 3\n",
        "best_state = None\n",
        "best_auc   = -np.inf\n",
        "best_vloss = np.inf\n",
        "stalled    = 0\n",
        "\n",
        "train_losses, val_losses = [], []\n",
        "train_accs,   val_accs   = [], []\n",
        "val_aurocs               = []\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    # ---- TRAIN ----\n",
        "    meta_fc.train(); gcn_conv.train(); concat_fc1.train(); output_fc.train()\n",
        "\n",
        "    # forward\n",
        "    x_gnn = F.relu(meta_fc(x_meta_tr))\n",
        "    x_gnn = F.relu(gcn_conv(x_gnn, edge_index_tr))\n",
        "    x_gnn = dropout(x_gnn)\n",
        "\n",
        "    x = torch.cat([x_txt_tr, x_gnn], dim=1)\n",
        "    x = dropout(F.relu(concat_fc1(x)))\n",
        "    logits = output_fc(x)                      # ‚Üê sin log_softmax\n",
        "\n",
        "    loss = criterion(logits, y_tr)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(            # ‚Üê evita explosi√≥n de gradientes\n",
        "        list(meta_fc.parameters())+\n",
        "        list(gcn_conv.parameters())+\n",
        "        list(concat_fc1.parameters())+\n",
        "        list(output_fc.parameters()),\n",
        "        max_norm=1.0\n",
        "    )\n",
        "    optimizer.step()\n",
        "\n",
        "    # m√©tricas train\n",
        "    with torch.no_grad():\n",
        "        pred_tr = logits.argmax(dim=1)\n",
        "        acc_tr  = (pred_tr == y_tr).float().mean().item()\n",
        "\n",
        "    # ---- VAL ----\n",
        "    meta_fc.eval(); gcn_conv.eval(); concat_fc1.eval(); output_fc.eval()\n",
        "    with torch.no_grad():\n",
        "        x_gnn_v = F.relu(meta_fc(x_meta_te))\n",
        "        x_gnn_v = F.relu(gcn_conv(x_gnn_v, edge_index_te))\n",
        "        x_gnn_v = dropout(x_gnn_v)             # dropout desactivado en eval\n",
        "\n",
        "        xv = torch.cat([x_txt_te, x_gnn_v], dim=1)\n",
        "        xv = dropout(F.relu(concat_fc1(xv)))\n",
        "        logits_v = output_fc(xv)\n",
        "\n",
        "        vloss = criterion(logits_v, y_te)\n",
        "        pred_v = logits_v.argmax(dim=1)\n",
        "        acc_v  = (pred_v == y_te).float().mean().item()\n",
        "\n",
        "        # AUROC (prob de clase 1)\n",
        "        probs_v = torch.softmax(logits_v, dim=1)[:, 1].detach().cpu().numpy()\n",
        "        y_true  = y_te.detach().cpu().numpy()\n",
        "        try:\n",
        "            auc_v = roc_auc_score(y_true, probs_v)\n",
        "        except ValueError:\n",
        "            auc_v = np.nan  # por si alguna clase falta en val\n",
        "\n",
        "    # guardar m√©tricas\n",
        "    train_losses.append(loss.item())\n",
        "    val_losses.append(vloss.item())\n",
        "    train_accs.append(acc_tr)\n",
        "    val_accs.append(acc_v)\n",
        "    val_aurocs.append(auc_v)\n",
        "\n",
        "    print(f\"Epoch {epoch:02d} | \"\n",
        "          f\"TrainLoss {loss.item():.4f} Acc {acc_tr:.3f} | \"\n",
        "          f\"ValLoss {vloss.item():.4f} Acc {acc_v:.3f} AUROC {auc_v:.3f}\")\n",
        "\n",
        "    # Step del scheduler\n",
        "    scheduler.step(vloss.item())\n",
        "\n",
        "    # ---- Early stopping por AUROC (desempate por menor val_loss) ----\n",
        "    improved = False\n",
        "    if not np.isnan(auc_v):\n",
        "        if (auc_v > best_auc) or (np.isclose(auc_v, best_auc) and vloss.item() < best_vloss):\n",
        "            improved = True\n",
        "    else:\n",
        "        # si AUROC no es computable, usa solo val_loss\n",
        "        if vloss.item() < best_vloss:\n",
        "            improved = True\n",
        "\n",
        "    if improved:\n",
        "        best_auc, best_vloss = float(auc_v), float(vloss.item())\n",
        "        best_state = {\n",
        "            \"meta_fc\": meta_fc.state_dict(),\n",
        "            \"gcn_conv\": gcn_conv.state_dict(),\n",
        "            \"concat_fc1\": concat_fc1.state_dict(),\n",
        "            \"output_fc\": output_fc.state_dict(),\n",
        "            \"optimizer\": optimizer.state_dict()\n",
        "        }\n",
        "        stalled = 0\n",
        "    else:\n",
        "        stalled += 1\n",
        "        if stalled >= PATIENCE:\n",
        "            print(\"üõë Early stopping (no mejora en AUROC/ValLoss).\")\n",
        "            break\n",
        "\n",
        "# Restaurar mejor checkpoint\n",
        "if best_state is not None:\n",
        "    meta_fc.load_state_dict(best_state[\"meta_fc\"])\n",
        "    gcn_conv.load_state_dict(best_state[\"gcn_conv\"])\n",
        "    concat_fc1.load_state_dict(best_state[\"concat_fc1\"])\n",
        "    output_fc.load_state_dict(best_state[\"output_fc\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Resultados del modelo h√≠brido (BETO + GCN)\n",
        "\n",
        "### 8.1 Curvas de entrenamiento\n",
        "\n",
        "Se visualizan las curvas de accuracy y p√©rdida durante 20 √©pocas para entrenamiento y validaci√≥n.\n",
        "\n",
        "- **Accuracy:** El modelo alcanza m√°s de 97% tanto en entrenamiento como validaci√≥n, con curvas estables desde la √©poca 6.\n",
        "- **Loss:** La p√©rdida cae r√°pidamente al inicio y se estabiliza en ambas fases, sin se√±ales de sobreajuste.\n",
        "\n",
        "> Estas curvas indican una convergencia r√°pida y un excelente poder de generalizaci√≥n.\n"
      ],
      "metadata": {
        "id": "G0D4sRySOyTl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Accuracy plot\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(train_accuracies, label=\"Train Accuracy\")\n",
        "plt.plot(val_accuracies, label=\"Validation Accuracy\")\n",
        "plt.title(\"Accuracy over epochs\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Loss plot\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(train_losses, label=\"Train Loss\")\n",
        "plt.plot(val_losses, label=\"Validation Loss\")\n",
        "plt.title(\"Loss over epochs\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xYhv5SQYN_w_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.1 Clasificaci√≥n por clase\n",
        "- **Accuracy total:** 0.97\n",
        "- **Macro F1-score:** 0.97\n",
        "\n",
        "### 8.2 Matriz de confusi√≥n\n",
        "\n",
        "- **Recall (Fake):** 0.95 ‚Üí El modelo detecta casi todas las noticias falsas.\n",
        "- **Precision (Fake):** 0.98 ‚Üí Los falsos positivos son m√≠nimos.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "twS4ypMcPCnN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vo852B9pShC0"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# Modo evaluaci√≥n\n",
        "text_fc.eval()\n",
        "meta_fc.eval()\n",
        "gcn_conv.eval()\n",
        "concat_fc1.eval()\n",
        "ln1.eval()\n",
        "output_fc.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    x_gnn_test = F.relu(meta_fc(torch.tensor(X_meta_test, dtype=torch.float32).to(device)))\n",
        "    adj_dense_test = torch.tensor(adj_enriched_test, dtype=torch.float32).to(device)\n",
        "    edge_index_test, _ = dense_to_sparse(adj_dense_test)\n",
        "\n",
        "    x_gnn_test = F.relu(gcn_conv(x_gnn_test, edge_index_test))\n",
        "    # üîπ sin dropout en test\n",
        "\n",
        "    text_input_test = torch.tensor(X_text_test, dtype=torch.float32).to(device)\n",
        "    x_text_test = F.relu(text_fc(text_input_test))   # üîπ proyecci√≥n como en train\n",
        "\n",
        "    x_concat_test = torch.cat([x_text_test, x_gnn_test], dim=1)\n",
        "\n",
        "    x_test = F.relu(concat_fc1(x_concat_test))\n",
        "    x_test = ln1(x_test)                             # üîπ normalizaci√≥n como en train\n",
        "    logits_test = output_fc(x_test)                  # üîπ sin log_softmax\n",
        "\n",
        "    pred_labels = logits_test.argmax(dim=1).cpu().numpy()\n",
        "\n",
        "# Reporte de clasificaci√≥n\n",
        "print(classification_report(y_test, pred_labels, target_names=[\"Real\", \"Fake\", \"Satira\"]))\n",
        "\n",
        "# Matriz de confusi√≥n\n",
        "cm = confusion_matrix(y_test, pred_labels)\n",
        "print(cm)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Real\", \"Fake\", \"Satira\"])\n",
        "disp.plot(cmap='Blues')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get prediction probabilities, not just classifications\n",
        "probs = F.softmax(logits_test, dim=1)\n",
        "fake_confidence = probs[:, 1].cpu().numpy()\n",
        "print(f\"Mean confidence for fake predictions: {fake_confidence[pred_labels==1].mean():.3f}\")\n",
        "print(f\"Mean confidence for real predictions: {fake_confidence[pred_labels==0].mean():.3f}\")"
      ],
      "metadata": {
        "id": "GvDcwuSKJlZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What real news is being misclassified as fake?\n",
        "false_positive_indices = np.where((y_test == 0) & (pred_labels == 1))[0]\n",
        "print(f\"False positive examples: {len(false_positive_indices)}\")\n",
        "# Examine these specific texts to understand the pattern"
      ],
      "metadata": {
        "id": "eEfYkG12JjZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53FG4dib-CcK"
      },
      "source": [
        "### 9. Conclusi√≥n\n",
        "\n",
        "El modelo h√≠brido supera significativamente al modelo textual puro (BETO + MLP), logrando un desempe√±o robusto en ambas clases. La integraci√≥n de se√±ales estructurales y ling√º√≠sticas permite mejorar la detecci√≥n de desinformaci√≥n, especialmente en casos m√°s sutiles.\n",
        "\n",
        "> Este resultado valida la hip√≥tesis central de la tesis: **el contexto estructural y narrativo aporta valor significativo al an√°lisis autom√°tico de veracidad**."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick BETO-only baseline test\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "lr_baseline = LogisticRegression(max_iter=1000)\n",
        "lr_baseline.fit(X_text_train, y_train)\n",
        "beto_baseline_acc = lr_baseline.score(X_text_test, y_test)\n",
        "print(f\"BETO-only baseline accuracy: {beto_baseline_acc:.4f}\")"
      ],
      "metadata": {
        "id": "QPr47TBzIcvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probs = F.softmax(logits_test, dim=1).detach().cpu().numpy()\n",
        "y_score = probs[:, 1]  # Probabilidad de ser FAKE"
      ],
      "metadata": {
        "id": "rlQt59v4jKSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# y_test: tus etiquetas verdaderas (0 = real, 1 = fake, 2 = satira)\n",
        "# y_score: tus probabilidades de ser FAKE (columna 1)\n",
        "# For multi-class, roc_auc_score needs a multi_class strategy\n",
        "roc_auc = roc_auc_score(y_test, probs, multi_class='ovr', average='macro')\n",
        "print(f\"AUROC: {roc_auc:.4f}\")"
      ],
      "metadata": {
        "id": "FEYMW3nMjMqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "# Assuming 'Fake' is class 1 and 'Real' and 'Satira' are other classes.\n",
        "# Plot ROC for 'Fake' class (label 1) against all other classes.\n",
        "# We need to convert the true labels to binary (1 for Fake, 0 otherwise)\n",
        "y_true_binary = (y_test == 1)\n",
        "# And use the probability of the Fake class\n",
        "y_score_fake = probs[:, 1]\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(y_true_binary, y_score_fake)\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "# Use the macro-averaged AUROC calculated previously\n",
        "plt.plot(fpr, tpr, label=f\"AUROC (Fake vs Rest) = {roc_auc:.2f}\", color=\"darkorange\", lw=2)\n",
        "plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"Curva ROC - Modelo H√≠brido (Fake vs Rest)\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LWGqD49mjOZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Benchmark del modelo\n",
        "\n"
      ],
      "metadata": {
        "id": "QnLH-hDOdLY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/jpposadas/FakeNewsCorpusSpanish.git"
      ],
      "metadata": {
        "id": "UoQBPe5ec1t_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns=[\"CATEGORY\", \"SOURCE\", \"HEADLINE\", \"TEXT\", \"LINK\"]\n",
        "test = pd.read_excel(\"/content/FakeNewsCorpusSpanish/test.xlsx\", index_col = None, usecols=columns)\n",
        "\n",
        "test.head()"
      ],
      "metadata": {
        "id": "AyA5aZe4tPH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = pd.DataFrame()\n",
        "df_test[\"TEXTO\"] = (test[\"HEADLINE\"].fillna(\"\").astype(str) + \". \" + test[\"TEXT\"].fillna(\"\").astype(str))\n",
        "df_test[\"LABEL\"] = test[\"CATEGORY\"].astype(str).str.lower().map({\"true\":0,\"real\":0,\"false\":1,\"fake\":1, \"satira\":2}).astype(int)\n",
        "df_test[\"FUENTE\"] = test[\"SOURCE\"].fillna(\"desconocido\").astype(str)\n",
        "\n",
        "# 3) FUENTE_COD (reusa el encoder del train si existe; si no, enc√≥dalo aqu√≠)\n",
        "if \"le_fuente\" in globals():\n",
        "    fmap = {cls:i for i,cls in enumerate(le_fuente.classes_)}\n",
        "    unk  = fmap.get(\"desconocido\", 0)\n",
        "    df_test[\"FUENTE_COD\"] = df_test[\"FUENTE\"].map(lambda x: fmap.get(x, unk)).astype(int)\n",
        "else:\n",
        "    df_test[\"FUENTE_COD\"] = LabelEncoder().fit_transform(df_test[\"FUENTE\"])\n",
        "\n",
        "# 4) Features num√©ricas crudas\n",
        "df_test[\"LONGITUD\"]  = df_test[\"TEXTO\"].str.split().str.len().astype(float)\n",
        "df_test[\"POLARIDAD\"] = df_test[\"TEXTO\"].apply(get_sentiment).astype(float)   # usa tu misma funci√≥n\n",
        "df_test[\"CLICKBAIT\"] = test[\"HEADLINE\"].fillna(\"\").astype(str).apply(clickbait_score).astype(float)\n",
        "\n",
        "df_test.head()\n"
      ],
      "metadata": {
        "id": "tfyyRGCzdPvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = MinMaxScaler()\n",
        "df_test[['LONGITUD_NORM', 'POLARIDAD_NORM', 'CLICKBAIT_NORM']] = scaler.fit_transform(\n",
        "    df_test[['LONGITUD', 'POLARIDAD', 'CLICKBAIT']]\n",
        ")\n",
        "\n",
        "# Crear matriz metadata final\n",
        "metadata_gnn = np.concatenate([\n",
        "    df_test['FUENTE_COD'].values.reshape(-1,1),\n",
        "    df_test[['LONGITUD_NORM', 'POLARIDAD_NORM', 'CLICKBAIT_NORM']].values\n",
        "], axis=1)\n"
      ],
      "metadata": {
        "id": "HrlYqvFvlQS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.head()"
      ],
      "metadata": {
        "id": "IkudPjIr4w9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_text_bench = generate_embeddings(df_test['TEXTO'])\n",
        "X_meta_bench = metadata_gnn.astype(np.float32)\n",
        "y_bench = df_test['LABEL'].to_numpy()\n",
        "\n",
        "# 2) Construir grafo (mismos umbrales que en tu c√≥digo)\n",
        "fuentes_b = X_meta_bench[:, 0]\n",
        "fechas_b = X_meta_bench[:, 1]\n",
        "polaridades_b = X_meta_bench[:, 3]\n",
        "\n",
        "sim_text_b = cosine_similarity(X_text_bench)\n",
        "adj_text_b = (sim_text_b > 0.8).astype(np.float32)\n",
        "adj_fuente_b = np.equal.outer(fuentes_b, fuentes_b).astype(np.float32)\n",
        "adj_fecha_b = (np.abs(fechas_b[:, None] - fechas_b[None, :]) <= 0.05).astype(np.float32) if np.std(fechas_b) > 1e-6 else np.zeros_like(adj_text_b)\n",
        "adj_polaridad_b = (np.abs(polaridades_b[:, None] - polaridades_b[None, :]) <= 0.1).astype(np.float32)\n",
        "\n",
        "adj_bench = np.clip(adj_text_b + adj_fuente_b + adj_fecha_b + adj_polaridad_b, 0, 1)\n",
        "np.fill_diagonal(adj_bench, 0)"
      ],
      "metadata": {
        "id": "9tUTT45SpKlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "meta_fc.eval(); gcn_conv.eval(); concat_fc1.eval(); output_fc.eval()\n",
        "with torch.no_grad():\n",
        "    # --- FIX DIMENSIONES METADATA (pad/recorte autom√°tico) ---\n",
        "    xmb = torch.tensor(X_meta_bench, dtype=torch.float32, device=device)\n",
        "    need = meta_fc.in_features\n",
        "    have = xmb.shape[1]\n",
        "    if have < need:\n",
        "        # rellenamos con 0.5 (valor neutro si usaste MinMaxScaler) al FINAL\n",
        "        pad = torch.full((xmb.size(0), need - have), 0.5, device=device)\n",
        "        xmb = torch.cat([xmb, pad], dim=1)\n",
        "    elif have > need:\n",
        "        # si sobran columnas, recorta\n",
        "        xmb = xmb[:, :need]\n",
        "\n",
        "    x_gnn_b = F.relu(meta_fc(xmb))\n",
        "    edge_index_b, _ = dense_to_sparse(torch.tensor(adj_bench, dtype=torch.float32, device=device))\n",
        "    x_gnn_b = dropout(F.relu(gcn_conv(x_gnn_b, edge_index_b)))\n",
        "    x_concat_b = torch.cat([torch.tensor(X_text_bench, dtype=torch.float32, device=device), x_gnn_b], dim=1)\n",
        "    x_b = dropout(F.relu(concat_fc1(x_concat_b)))\n",
        "    logits_b = F.log_softmax(output_fc(x_b), dim=1)\n",
        "    y_pred_b = logits_b.argmax(dim=1).cpu().numpy()\n"
      ],
      "metadata": {
        "id": "XrvmxoSppQR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_bench, y_pred_b, target_names=[\"Real\", \"Fake\", \"Satira\"]))"
      ],
      "metadata": {
        "id": "HET4QrGopbT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm_benchmark = confusion_matrix(y_bench, y_pred_b)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm_benchmark, display_labels=[\"Real\", \"Fake\", \"Satira\"])\n",
        "disp.plot(cmap='Blues')"
      ],
      "metadata": {
        "id": "sSBXjV7HphaH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}